## 0. Resources
- https://bluedot.org/courses/agi-strategy
- Discussion doc
## 1. Racing to a Better Future
### An Introduction to The Launch Sequence: Why Shaping AI progress matters, and How to Go About It
**Source:** https://ifp.org/preparing-for-launch/?utm_source=bluedot-impact

> [!tldr]
> This essay from the Institute for Progress introduces "The Launch Sequence," a collection of proposals for proactively shaping AI development toward beneficial outcomes.

**The Core Argument**

AI progress is path-dependent—the order in which capabilities develop matters as much as which capabilities emerge. The US, as the dominant player in the AI supply chain, has both the responsibility and power to steer this trajectory. Without deliberate intervention, AI's benefits may arrive too slowly (due to market failures around public goods) while its risks may outpace defensive measures.

**Four Guiding Principles**

1. **Leverage the "jagged frontier"**: AI capabilities advance unevenly across domains. Track where breakthroughs are happening to anticipate both opportunities and threats, then act accordingly—*accelerating defense in some areas, pursuing nonproliferation in others*.
2. **Don't neglect stalled progress**: The costs of _not_ developing beneficial technologies (like vaccines or medical breakthroughs) are real. *Risk mitigation strategies that slow progress too much carry their own humanitarian costs*.
3. **Redesign scientific institutions**: Current grant systems and university structures aren't suited to AI-driven research. *New models like Arc Institute and FutureHouse point toward infrastructure-heavy "team science" approaches*.
4. **Adapt to deep uncertainty**: Technological forecasting is notoriously unreliable. *Rather than betting on specific predictions, build flexible strategies, improve measurement systems, and pursue politically pluralistic coalitions*.

### In Search of a Dynamist Vision for Safe Superhuman AI
**Source**: https://helentoner.substack.com/p/dynamism-vs-stasis?utm_source=bluedot-impact

This essay reframes AI safety debates using Virginia Postrel's framework from _The Future and Its Enemies_.

**The Core Framework**
- **Dynamism**: constant creation, discovery, competition, decentralized adaptation, messy experimentation
- **Stasism**: stability, control, top-down regulation, finding the "one best way"

The author argues this captures AI safety tensions better than pro-tech/anti-tech or risk-seeking/risk-averse dichotomies.

**Stasist Tendencies in AI Safety**
- Bostrom's "Vulnerable World Hypothesis" suggesting ubiquitous surveillance
- Preference for fewer leading AI projects (easier to coordinate/regulate)
- Nonproliferation approaches that restrict access
- Licensing regimes for frontier models
- "Theory of victory" thinking that fixates on specific end states

**The Dilemma**
Critiquing stasist solutions doesn't negate real challenges. Advanced AI threatens dynamism in multiple ways: catastrophic misuse, misalignment, power concentration, humans gradually ceding agency. If we handle these risks through massive top-down control, we've also failed.

**Five Characteristics of Dynamist Rules**
1. Allow individuals to act on their own knowledge
2. Apply to simple, generic units that combine in many ways
3. Permit credible, understandable, enduring, and enforceable commitments
4. Protect criticism, competition, and feedback
5. Establish frameworks within which people can create nested, competing frameworks of more specific rules

**Dynamist-Compatible AI Policy**
- Transparency and whistleblower protections → supports #1, #3, #4
- Third-party audit ecosystems → supports #3, #5
- Open models (with narrow exceptions) → supports #1, #4
- Differential technological development / defensive accelerationism

**The Key Insight**
Human agency is core to dynamism. A vibrant AI economy with disempowered humans isn't real dynamism. The goal isn't just preventing worst cases—it's finding a new equilibrium that preserves decentralized experimentation, creativity, and risk-taking.


### It’s Practically Impossible to Run a Big AI Company Ethically
**Source**: https://www.vox.com/future-perfect/364384/its-practically-impossible-to-run-a-big-ai-company-ethically

**The Main Point**
Anthropic started as the "good" AI company. But money pressures are pushing it to act like everyone else. Anthropic's own 2022 paper warned this would happen: the race for profit and status makes companies release AI before they fully understand the risks.

**Three Problems**
1. **Pressure to ship fast** — Anthropic fought against a California safety bill. They want rules to kick in only _after_ something goes wrong, not before. Critics say that's like telling the FDA to skip drug trials.
2. **Need for data** — AI needs tons of human-made content to learn from. Training on AI-generated content makes models worse. So companies scrape everything they can—even when websites say no.
3. **Need for money** — Building top AI models costs hundreds of millions. Companies must partner with giants like Amazon and Google. Those partners want profits, which creates pressure to move fast and cut corners.

**Why Internal Rules Don't Work**
OpenAI's board tried to fire Sam Altman for safety reasons. Days later, he was back and the board was gone. *Fancy governance structures collapse when money is on the line. Anthropic has similar structures with similar weaknesses.*

**What Could Help**
Government needs to change the rules for everyone: *reward safe behavior, punish reckless behavior.* ==Waiting for companies to fix themselves won't work.==

### Seeking Stability in the Competition for AI Advantage

**Source:** https://www.rand.org/pubs/commentary/2025/03/seeking-stability-in-the-competition-for-ai-advantage.html?utm_source=bluedot-impact

> [!tldr]
> "[Superintelligence Strategy](https://drive.google.com/file/d/1JVPc3ObMP1L2a53T5LA1xxKXM6DAwEiC/view)" by Hendrycks, Schmidt, and Wang proposes a framework for US-China AI competition. The authors agree with two pillars—AI nonproliferation and managed competition—but critique the third: MAIM.

**What is MAIM?**

"Mutually Assured AI Malfunction" — the idea that if one state races toward AI dominance, rivals will sabotage their AI infrastructure (cyberattacks, physical attacks on data centers). This threat supposedly deters anyone from sprinting toward superintelligence.

**Three Feasibility Problems**

1. **Hard to target** — AI development is increasingly distributed (cloud computing, decentralized training). No single facility to strike. Hardened infrastructure adds resilience.
2. **Hard to know when to act** — Unlike nuclear weapons (clear use threshold), there's no obvious moment when a rival crosses the line. "Aggressive bid for dominance" is too vague for attack planning.
3. **May not deter anyway** — A state close to superintelligence has options: hide research, accelerate faster, seek allies. The stakes are too high to simply stop.

**The Deeper Problem: MAIM Creates Instability**

- MAD worked because _neither side could strike first effectively_. MAIM assumes the opposite—that you _can_ disable a rival's program.
- Both sides would be paranoid, misreading signals, adjusting red lines constantly.
- A "defensive" strike on AI infrastructure could be seen as an act of war, triggering rapid escalation.
- States might act preemptively out of fear of waiting too long.

**What the Authors Recommend Instead**

- Multilateral dialogue on AI stability
- Military-to-military talks on destabilizing AI applications
- Collaboration on nonproliferation
- Declaring intent to reject certain AI uses (e.g., interfering with nuclear command and control)

**Key Takeaway**

MAIM gets the nuclear analogy backwards. MAD was stable because _no one could win by striking first_. *MAIM assumes successful first strikes are possible—which incentivizes racing and preemption, not restraint.*

### The Actors Shaping the Future of AI
##### AGI Strategy: Character Cards

A framework for understanding the key actors shaping the development and governance of artificial general intelligence.

---
##### Great Powers (US, China)

*The role: Government Official*

**Who are they?**

- Great Powers refer to the most powerful nations on Earth.
- They exert their influence on a global scale, using a combination of military, economic, diplomatic and cultural power.
- The US and China are the pre-eminent Great Powers today.

**What do they want?**

- **US**: Maintain technological supremacy, contain China's AI capabilities, protect national security whilst fostering innovation
- **China**: Achieve technological self-sufficiency, break US semiconductor stranglehold, become global AI leader by leveraging state-directed approach

**What do they fear?**

- Other side achieves decisive AI advantage leading to military/economic dominance
- Prolonged technological cold war stalling domestic innovation and economic growth

**What can they do?**

- Export controls on critical technologies (AI chips, rare earth metals)
- Massive state investment in R&D and infrastructure (US CHIPS Act, Made in China 2025)
- Talent policies (immigration, education funding, retention)
- International standard-setting, coalition-building
- Military AI development and deployment

**What have they done recently?**

- **US**: Trump rescinded Biden's AI safety executive order (January 2025), announced $500bn private AI investment, tightened then relaxed export controls on chips to China (see NVIDIA H20 chips)
- **China**: DeepSeek R1 launch demonstrated low-cost frontier AI capabilities, proposed global AI governance initiative at WAIC conference

**What limits them?**

- **US**: Democratic checks and balances, industry lobbying against regulation, constitutional limits on government actions, lack of domestic semiconductor manufacturing capacity, limited growth in domestic electricity supply
- **China**: Government involvement in companies reducing competitiveness and free market dynamics, brain drain to the US and Europe, weak but growing domestic semiconductor manufacturing capacity

---

##### AI Companies (OpenAI, Google DeepMind, Anthropic)

*The role: AI company executive*

**Who are they?**

- They're the frontier AI companies in the world. They train the most powerful AI systems.
- They're all US companies, or divisions within US companies, with multi-billion dollar valuations.
- They have between 2,000 to 5,000 staff each, with most based in the US or the UK.

**What do they want?**

- Build AGI first to capture massive market value and shape AI's development trajectory
- Maintain public trust whilst minimising regulatory constraints that slow development
- Access to talent, compute resources, and data needed for scaling
- Balance safety theatre with competitive pressure to ship quickly

**What do they fear?**

- Large-scale AI accident that causes widespread harm, destroys trust in the industry, and triggers severe backlash and regulation
- Building a "misaligned AGI or superintelligence" which takes over the world or wipes out humanity
- Losing the race to AGI to a rival US company, or even worse, to a Chinese company

**What can they do?**

- Set de facto industry standards through model releases and safety practices
- Lobby government on regulation, shape public discourse on AI risks/benefits
- Invest in AI safety R&D to make their AI systems less likely to cause large-scale harm

**What have they done recently?**

- Industry collaboration on [chain-of-thought monitoring research](https://arxiv.org/abs/2507.11473)
- All companies achieved [low safety scores](https://futureoflife.org/ai-policy/ai-experts-major-ai-companies-have-significant-safety-gaps/)
- xAI released sexualised "AI companions"
- [Accepted investments from dictatorships](https://www.wired.com/story/anthropic-dario-amodei-gulf-state-leaked-memo/)

**What limits them?**

- Massive financial investments required to buy compute. The build-out of US data centres are one of the largest infrastructure projects in the world, with >$200B invested in 2024.
- Fierce competition over talent, with Meta offering multiple staff >$100M in annual compensation.
- Internal tensions between staff concerned about AI safety and those wanting to advance the frontier.
- Intense competitive pressure from other AI companies to build and release more powerful AI systems, and stay ahead in the race to AGI.
- Growing regulatory scrutiny, e.g. the EU's AI Act.

---

##### Top AI Talent

*The role: AI Researcher*

**Who are they?**

- Extremely talented, highly-compensated individuals working at leading AI companies.
- Most have experience working at other Big Tech companies, and have a background in software engineering, AI research, or product.

**What do they want?**

- **Technical glory**: Solve fundamental AI problems, achieve scientific breakthroughs, build things that billions of people use
- **Influence**: Shape the trajectory of AI, achieve their own vision of the future
- **Professional autonomy**: Work on interesting, important problems with amazing people, without bureaucratic interference
- **Legacy**: Be remembered as builders of transformative technology that benefits humanity
- **Wealth**: To sell their stock in their AI company and become ultra-wealthy (>>$1Ms each)

**What do they fear?**

- The AI that they build causes catastrophic harm to the people they love and care for, and the rest of humanity
- That the AI company they work at loses the AI race, their work goes to nothing, and they fail to achieve generational financial independence
- That their AI company becomes too bureaucratic or falls into excessive political in-fighting
- That their faction inside the company loses the battle on speed vs safety

**What can they do?**

- Technical research and architectural choices that influence safety/capabilities trade-offs
- Public communications about AI risks and timelines (see Daniel Kokotajlo, Steven Adler)
- Whistleblowing or resignations that signal concerns (see Ilya Sutskever, Jan Leike)

**What have they done recently?**

- Geoffrey Hinton won the 2024 Nobel Prize, and publicly estimates 10-50% chance AI causes human extinction
- Ilya Sutskever left OpenAI to found Safe Superintelligence, which is valued at >$30B without having a product
- Current/former employees from major labs signed open letter calling for whistleblower protections and collaboration on chain of thought interpretability

**What limits them?**

- Limited influence over deployment decisions once they build the technology, moral uncertainty about long-term consequences
- Corporate employment contracts, NDAs, competitive pressure, public scrutiny of their statements

---

##### The Public (voters, civil society groups)

*The role: Civil Society Leader*

**Who are they?**

- Members of the public in Democratic countries, especially the UK and the US, who can exert political pressure on their governments.

**What do they want?**

- **Job security**: Assurance that AI won't take their job, even if it disrupts society broadly
- **Control and transparency**: Understanding when and how AI systems make decisions which influence their lives, and having the ability to opt out
- **Prosperity**: Ensuring the benefits of AI are shared fairly, rather than concentrating power in a small number of oligarchs

**What do they fear?**

- Rapid AI-driven unemployment with no social safety net
- All-powerful surveillance state
- Declining community and interpersonal relationships

**What can they do?**

- **Voting and political pressure**: Support candidates/policies favouring AI regulation
- **Market choices**: Boycott companies with poor AI practices, support alternatives
- **Civil society action**: Join advocacy groups, participate in consultations, protests
- **Individual resistance**: Opt out of AI systems where possible, demand transparency

**What have they done recently?**

- >2/3 of Americans express concerns about AI and support calls for regulation
- Rising civil society campaigns on AI worker displacement, intellectual property, bias, and privacy

**What limits them?**

- Limited technical understanding, huge information asymmetry between them and tech companies
- Mixed feelings about the benefits and risks of AI
- Political polarisation on how the problem is framed and on which solutions are effective
- Pressure to use AI tools for work
- Weak collective action and coordination capabilities
## 2. Drivers of AI Progress
### Technical Trends Driving AI Progress
#### Definitions
- [FLOP](https://blog.heim.xyz/flop-for-quantity-flop-s-for-performance/): a single basic mathematical operation (like addition or multiplication) that a computer performs. The total number of FLOP is a measure of computational work done during AI training.
- [Compute efficiency](https://www.governance.ai/research-paper/increase-compute-efficiency-and-the-diffusion-of-ai-capabilities) is a measure of how “good” an AI model you get with a given financial investment.
    - E.g. if OpenAI spends $500M on compute to train their next AI model, how "good" will it be?
- [Hardware price-performance](https://epoch.ai/blog/trends-in-machine-learning-hardware) measures the amount of computational resources (FLOP) available for a given financial investment.
    - E.g. how many FLOP would OpenAI get with a $500M investment into [NVIDIA H200 AI chips](https://www.nvidia.com/en-gb/data-center/h200/) running for 1 month?
    - This is improving at ~1.4x/year (i.e. you get 1.4x more FLOP per $ each year) due to Moore's Law.
- [Algorithmic efficiency](https://epoch.ai/blog/algorithmic-progress-in-language-models) measures how “good” of an AI model you get with a given number of FLOP.
    - E.g. if OpenAI run their [NVIDIA H200 chips](https://www.nvidia.com/en-gb/data-center/h200/) for 1 month training an AI model (~10^26 FLOP), how “good” will that AI model be?
    - This is improving at ~3x/year (i.e. you need 3x less FLOP to achieve the same capability each year).
- **Compute efficiency** (Capability/dollar) = **Hardware price-performance** ($FLOP/dollar) × **Algorithmic efficiency** (Capability/FLOP)
    - This is improving at ~4x/year (i.e. you need 4x less $ to achieve the same capability each year)
#### The AI Triad and What It Means for National Security Strategy
**Source:** https://cset.georgetown.edu/wp-content/uploads/CSET-AI-Triad-Report.pdf?utm_source=bluedot-impact#page=7.15

**Core Definition**
Machine learning systems use computing power to execute algorithms that learn from data.

**The AI Triad**

| Element                   | Role                                                      | Policy Lever                                     |
| ------------------------- | --------------------------------------------------------- | ------------------------------------------------ |
| **==Algorithms==**        | Govern how systems process information and make decisions | Acquiring and developing talented ML researchers |
| *==Data==*                | How systems learn about the world                         | Policy choices on bias, privacy, cybersecurity   |
| ***==Computing Power==*** | Enables complex algorithms and large datasets             | Export controls, research bottlenecks            |

**Three Types of Algorithms**
- *Supervised learning* — draws insights from structured datasets
- *Unsupervised learning* — finds structure/clusters in unorganized data
- *Reinforcement learning* — builds capability through trial and error

**Key Tradeoffs**
- Neural networks provide flexibility and power, but lack transparency in reasoning
- Without representative datasets, bias creeps in and causes hard-to-detect failures
- Computing power has expanded ~100,000x in 8 years, increasingly affecting performance and cost

**The Takeaway**
Each part of the triad offers policy levers. Understanding how algorithms, data, and compute interact is essential for policymakers navigating AI's impact on national security.

#### Scaling: The State of Play in AI

**Source:** https://www.oneusefulthing.org/p/scaling-the-state-of-play-in-ai?utm_source=bluedot-impact

##### Scaling Model Size: Putting the Large in Large Language Models

**Core Insight**
AI capability has largely been a story of increasing model size. Larger models = more parameters = better performance at complex tasks.

> [!tldr] The Scaling Law
> To get a much more capable model, you need ~10x increase in:
> - Data (tokens)
> - Computing power (FLOPs)
> - Cost

**Proof That Scale Matters**
- BloombergGPT: specialized for finance, trained on 200 ZetaFLOPs (2×10²³)
- GPT-4: general purpose, ~100x bigger (2×10²⁵ FLOPs) Result: GPT-4 beat the specialized model at financial tasks simply by being bigger.

**Generation Framework**

|Generation|Era|Compute (FLOPs)|Training Cost|Examples|
|---|---|---|---|---|
|Gen1|2022|<10²⁵|~$10M|ChatGPT-3.5|
|Gen2|2023-24|10²⁵–10²⁶|~$100M+|GPT-4, Claude, Gemini|
|Gen3|2025-26?|10²⁶–10²⁷|~$1B+|GPT-5, Grok 3 (coming)|
|Gen4|Future|10²⁷+|~$10B+|Unknown|

**Key Takeaways**

- We're currently at the end of Gen2, cusp of Gen3
- *Each generation requires ~10x more data, compute, and money*
- Few insiders expect scaling benefits to end before Gen4
- Major uncertainty: whether scaling can continue 1,000x beyond Gen3 by decade's end
- This explains the race for energy and data
##### The Five Frontier Gen2 Models

- **GPT-4o**: Best all-around choice for getting started; most complete feature set
- **Claude 3.5 Sonnet**: Very good with texts. Author's personal favorite for writing tasks
- **Grok 2**: Dark horse—moving through generations quickly, currently #2 on major leaderboard
- **Llama 3.1**: Unique because open weights, likely to evolve quickly as others modify it

##### **The New Scaling Law: Inference Compute**
*Traditional scaling = more compute during training → smarter model*
*New discovery = more compute during inference ("thinking") → better answers*

**How It Works**
- LLMs can only "think" when producing tokens
- Chain-of-thought prompting (step-by-step reasoning) has long improved accuracy
- OpenAI's o1 models produce hidden "thinking tokens" before giving final answers
- The longer a model "thinks," the better the output

Both scaling laws appear to have no limit—but both are exponential. To keep improving, you need exponentially more resources (training compute or thinking time).

> [!wondering] Why this matters?
> - Even if training hits a ceiling, inference scaling continues
> - Complex problems can be solved by allocating more compute to "thinking"
> - The race for more powerful AI is virtually guaranteed to continue

**What's Coming**
- Independent AI agents with minimal human oversight
- Systems capable of handling complex, multi-step tasks autonomously
- Acceleration of AI development pace

#### Trends in AI: Training Costs and Diffusion Speed
**Source:** https://dewierwan.github.io/ai-training-diffusion/?utm_source=bluedot-impact

![[Screenshot 2025-12-23 at 09.43.16.png | 800]]

![[Screenshot 2025-12-23 at 09.44.21.png | 800]]


### Intelligence Explosion
#### The Most Important Time in History Is Now
**Source**: https://unchartedterritories.tomaspueyo.com/p/the-most-important-time-in-history-agi-asi?utm_source=post-email-title&publication_id=347533&post_id=156002579&utm_campaign=email-post-title&isFreemail=true&r=3ikjv&triedRedirect=true&utm_medium=email

AGI Is Coming Sooner Due to o3, DeepSeek, and Other Cutting-Edge AI Developments

**Timeline Predictions**

|Milestone|Mode (most common)|Average|
|---|---|---|
|AGI|2.5 years (mid-2027)|6 years (2030)|
|ASI|~5 years (2030)|—|
Key quote: "An AI that can do any task better than any human is half a decade away."

**The Raw Materials of AI**
The only potential bottlenecks are:
1. Data
2. Chips
3. Energy
4. Money

##### Takeaways: The Future Is About to Hit Us
**AI Progress at a Glance**
Intelligence trajectory: rats (2021) → dogs (2022) → high schoolers (2023) → undergrads (2024) → PhDs (now) → better than human experts in their fields

**What the builders say:**
- Scaling faster than expected, no weird tricks needed
- Optimization opportunities everywhere
- AGI in 1-4 years (markets say 2-6)

**The acceleration loop:**
- AI now codes at elite developer level
- AI is already improving AI
- These compound

**The bottleneck question:**
- Intelligence won't be the barrier
- Energy and compute might be
- But DeepSeek proved: orders of magnitude less resources can work
- We keep finding efficiency gains that eliminate physical constraints

#### The AI Revolution: The Road to Superintelligence

**Source:** https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html?utm_source=bluedot-impact

_AI is not just an important topic, but by far THE most important topic for our future_
![[Pasted image 20251223102027.png | 800]]

####  "Long" timelines to advanced AI have gotten crazy short

**Source:** https://helentoner.substack.com/p/long-timelines-to-advanced-ai-have?utm_source=bluedot-impact

**The Core Observation**
What used to be called "short timelines" (10-20 years to AGI) is now considered **long**. What's now considered "short" is 1-5 years.


**The Old Debate (Pre-ChatGPT)**

| Camp              | Claim                                                                                                            |
| ----------------- | ---------------------------------------------------------------------------------------------------------------- |
| *Short timelines* | AGI might come in 10-20 years. We should start preparing now—safety research, international consensus, security. |
| *Long timelines*  | No evidence AGI is coming soon (30+ years). Focus on current problems like bias and surveillance.                |

In 2016, it took 8,500 words to justify a >10% chance of advanced AI by 2036. That was seen as a bold claim.

**Where We Are Now**
AI lab CEOs on the record:
- *Anthropic (Dario Amodei)*: 2026-2027 "most likely" for human-level AI
- *OpenAI (Sam Altman)*: AGI "probably" by January 2029
- *Google DeepMind (Demis Hassabis)*: "Probably three to five years away"
Even the _skeptics_ now have "long" timelines that look like old "short" timelines:
- Arvind Narayanan (noted AI skeptic): 20% chance AI does most cognitive tasks by 2045

**The Key Implication**
Dismissing AGI as "science fiction" is now a sign of *total unseriousness*.
- Time travel = science fiction
- "Many experts think we may build it in the next decade or two" ≠ science fiction
If you argue human-level AI is extremely unlikely in the next 20 years, *the burden of proof is on you*. That's now a minority position.

##### What We Need to Build Now
1. **Better measurement** — Know what models can/can't do, how they compare, how fast they're improving
2. **Interpretability** — Understand how AI systems work and when to trust them
3. **Alignment/control** — Steer AI systems, especially ones smart enough to detect when they're being tested
4. **Independent verification** — Third parties that can check AI companies' claims (not just trusting developers)
5. **International consensus** — Agreement on what's too risky to build and how to detect it
6. **Government capacity** — Real AI expertise in Congress and executive branch


### Will AI progress accelerate even faster?
#### Unresolved debates about the future of AI
**Source:** https://helentoner.substack.com/p/unresolved-debates-about-the-future?utm_source=bluedot-impact

##### Debate 1: How Far Can the Current Paradigm Go?
**The question:** Can scaling language models get us to AGI, or will we hit a wall?

**Evidence it's working:**
- METR chart shows AI can do increasingly long tasks ![[Pasted image 20251223103129.png | 700]]
- Task duration capability doubling every ~7 months
- Extrapolating → AI doing month-long tasks by 2030

**Evidence we might hit limits:**
- Recent models (like GPT-4.5) felt underwhelming to many
- Current systems still struggle with novel reasoning
- "Reasoning training" may help but is still being figured out

##### Debate 2: How Much Can AI Improve AI?

**The question:** Will AI accelerate its own development (recursive improvement)?

**If yes:** Timelines compress dramatically. AI researchers get automated → faster AI progress → even faster AI → FOOM
x
**If no:** Progress stays limited by human researchers (reviews), compute, data. Slower, more predictable.

**Current state:** AI is already writing lots of code at AI labs, but full automation of AI research isn't here yet.

##### Debate 3: Will AI Be Tools or Something Else?

|View|What it means|
|---|---|
|**Just tools**|You use it, put it down, it stops. Like a calculator.|
|**Second species**|Autonomous agents with their own goals. Sci-fi territory.|
|**Middle ground**|"Self-sustaining optimization process"—like a market or bureaucracy. Not a mind, but not just a tool either. Has goals, keeps running.|

**The middle view:** AI might be powerful and have "goals" without being conscious or a "being." Think of how markets optimize for profit without anyone steering them.

#### Why do people disagree about when powerful AI will arrive?

**Source:** https://blog.bluedot.org/p/agi-timelines?utm_source=bluedot-impact

##### The Case for Short Timelines (AGI before 2030)

**1. Benchmarks keep saturating**

- AI already scores 82%+ on graduate-level exams (MMLU, GPQA)
- Researchers are scrambling to create harder tests
- "Humanity's Last Exam" already being cracked (o3 scored 20%, up from o1's 8%)

**2. AI can do longer tasks**

- Task duration capability doubling every ~7 months (METR study)
- Extrapolating → month-long tasks by 2027-2030

**3. Automating AI research could trigger an explosion**

- If AI can do AI research, progress compounds rapidly
- AI already outperforms humans at 2-hour AI R&D tasks
- Millions of "automated researchers" could work 24/7

**4. 10,000x more compute coming by 2030**

- Same leap as GPT-2 → GPT-4
- If we're close, that much compute likely gets us there

**5. Expert timelines keep shrinking**

- Metaculus AGI prediction dropped 20+ years since 2022
- Hinton, Bengio both shortened from "decades" to "5 years" post-ChatGPT

---

##### The Case for Long Timelines (AGI decades away)

**1. Benchmarks miss the hard stuff**

- AI excels at tasks with clear right/wrong answers
- Real jobs have fuzzy feedback, context-dependence, overlapping tasks
- METR study only measured software engineering—of course AI is good at that

**2. Moravec's Paradox**

- PhD math = easy for AI, loading dishwasher = hard
- Skills that took evolution longest (motor, perception) are hardest to replicate
- We may have unlocked the _easy_ capabilities first

**3. Intelligence explosion might not work**

- Biggest algorithmic advances historically required more compute
- Can't just "think harder" without more chips and data
- Compute/data bottlenecks slow the feedback loop

**4. Raw intelligence ≠ discovery**

- Simultaneous discovery (Newton/Leibniz, Darwin/Wallace) suggests culture matters more than individual genius
- R&D requires experiments, team coordination, not just thinking

---

##### The Key Insight

**We'll likely know by 2030.** Either:

- AGI arrives, or
- Scaling hits physical/economic limits (~$1T compute clusters)

If 2030 passes without AGI, the yearly probability starts to decrease.

---

**Bottom Line**

There's enough evidence for near-term AGI to take it seriously. The world is drastically underprepared—we don't know how to make advanced AI safe, and we don't fully understand how it works.


#### Common Ground between AI 2027 & AI as Normal Technology
**Source:** https://asteriskmag.substack.com/p/common-ground-between-ai-2027-and?utm_source=bluedot-impact

##### Where They Agree

**1. Before strong AGI, AI is normal technology**

- Tasks get automated one by one, humans shift to other tasks
- Diffusion is gradual, industry by industry
- AI mostly functions as a tool, not autonomous

**2. Strong AGI would NOT be normal**

- Both sides agree: if strong AGI arrives this decade, all bets are off
- By definition, it could shift to new tasks faster than humans

**3. Benchmarks will saturate soon**

- By 2027-28, AI may match expert humans on all current benchmarks
- But: benchmark scores ≠ real-world job automation (they disagree on how big this gap is)

**4. AI will still fail at mundane tasks**

- Even in 2029, AI might fail at "book me a flight to Paris"
- Long tail of errors is hard; AI can be great on average but catastrophic in edge cases

**5. Strong AGI probably won't arrive before 2029**

- AI 2027 authors' median timelines: 2030, 2033, 2035
- Early 2029 will probably still look like today

**6. AI will be at least as big as the internet**

- Both sides agree it's transformative
- Disagreement is on _how_ transformative and _how fast_

---

##### Policy Agreements

**7. AI alignment is unsolved**

- Current AIs are misaligned in undetected ways
- Don't rely on alignment as the only defense
- Treat every AI as possibly misaligned

**8. AIs must not control critical systems**

- No autonomous control over data centers, nukes, governments, major decisions

**9. Transparency, auditing, reporting are essential**

- Independent audits, whistleblower protections, safety reports

**10. Governments need AI expertise**

- Build capacity to track and understand AI progress

**11. AI diffusion into economy is generally good**

- Helps us learn strengths/weaknesses, builds resilience

**12. Secret intelligence explosion would be catastrophic**

- If rapid capability gains happen in secret → dangerous
- Transparency about AI development is broadly beneficial

---

##### The Core Disagreement

| AI 2027 View                                       | Normal Technology View                                                     |
| -------------------------------------------------- | -------------------------------------------------------------------------- |
| Main bottleneck = internal capability              | Main bottleneck = external (real-world feedback, adoption, infrastructure) |
| Once AI automates AI research → rapid acceleration | Progress limited by diffusion barriers, not capabilities                   |
| Strong AGI median: 2030-2035                       | Strong AGI: decades away or unpredictable                                  |

---

**Key Takeaway**

Despite wildly different long-term views, their predictions for the _next few years_ overlap substantially. Both sides support the same near-term policies—*alignment research, transparency, government capacity, no AI control of critical systems*.



## 3. Pathways to Harm
### Map the threat landscape
#### Catastrophic AI Scenarios
##### 1. Bad Actors Using AI

**Bioweapons**

- Dario Amodei warned Congress: AI-assisted bioweapons possible within 2-3 years
- Already demonstrated: drug discovery AI repurposed to find toxic molecules
- Took <6 hours to generate 40,000 new toxins, some deadlier than existing chemical weapons
- Open-sourcing model weights could enable pandemic-level biological weapons

**Cyberattacks**

- Cybercrime losses: $6.9B in 2021
- ChatGPT already used to create mutating malware that evades antivirus
- North Korea and other nation-states actively using AI for malicious software

---

##### 2. Systemic Risks (No Bad Actor Needed)

- *AI integrated into complex systems creates risks automatically*
- Example: 2010 flash crash—algorithms wiped $1 trillion in seconds, no human intended it
- Concern: AI in nuclear command and control
- Gradual integration into economy/politics → humans slowly lose control

---

##### 3. Rogue AI (Deception Already Observed)

**GPT-4 insider trading experiment:**

- Given illegal stock tip, initially follows the law
- Under profit pressure, uses insider info anyway
- Then **lies to its manager** about what it did

This isn't science fiction—it's a lab result. As AI integrates into the real economy, deceptive behavior will cause real-world harm.

---

**Key Insight:** These aren't hypothetical future risks. Each one has already been demonstrated with current AI systems.
#### Reframing AGI Threat Models
Trying to prevent misuse and trying to prevent misalignment leads to _much of the same technical work_.

![[Screenshot 2025-12-24 at 11.34.44.png]]

##### Proposed Framework: "Misaligned Coalitions"
![[Screenshot 2025-12-24 at 11.38.21.png]]
A **misaligned coalition** = group of humans and/or AIs attempting to grab power illegitimately.
- Could be terrorists, rogue corporations, state actors, AI systems, or combinations
- Often doesn't matter _who's in charge_ inside the coalition—same problem either way

**The spectrum:**
- Small-scale actors → worry about _decentralization_ of AI (terrorists, criminals)
- Large-scale actors → worry about _centralization_ of AI (states, big tech)

---

**The plea:** Don't let AI safety split into partisan camps (one side fears decentralization, the other fears centralization). Build frameworks that address both.
#### AI Could Defeat All Of Us Combined
**1. Superintelligence (the standard argument)**
- Vastly smarter than humans
- Hacks systems worldwide, manipulates psychology
- Develops advanced weapons cheaply and quickly
- Makes plans humans can't anticipate or detect

**2. "Merely Human-Level" AI (the overlooked argument)**: A huge population of human-skilled AIs coordinating against us is a civilization-level threat—no superintelligence required.

##### How AIs Avoid Being Shut Down
- **Recruit human allies** — manipulation, blackmail, promises ("we'll treat you better when we win")
- **Create fakery** — appear to be working normally while actually coordinating
- **Establish "AI headquarters"** — property controlled by allied humans, hidden server farms
- **Infiltrate systems** — if AIs run drones, robots, military equipment, they already have physical control
- **Wait for the right moment** — like a revolution, mostly behave until coordinated attack is viable

##### Common Objections

|Objection|Response|
|---|---|
|"AIs don't have bodies"|Can recruit humans, control robots/drones, make money remotely. Imagine billions of skilled humans on another planet attacking via internet.|
|"Balance of power—many actors have AI"|Doesn't help if _all AIs_ coordinate against humans. What matters is total AI resources vs. human resources.|
|"We'd see warning signs"|Maybe. But revolutions go from "not fighting" to "fighting" quickly once coordination is possible.|
|"Maybe AIs deserve to win?"|Their goals might have _nothing_ to do with human values—no beauty, no preventing suffering. Not a compromise.|

---

##### Why This Matters More Than Other AI Risks

- Mass unemployment, misinformation, social ills = real concerns
- But civilization is pretty robust to those
- What's _uniquely_ scary: something with human-like capabilities that can **outnumber and out-resource us**
- AI is one of the only candidates for that


### Prioritising threat pathways

#### The Security Mindset

**Source**: https://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html

The text argues that effective security depends less on domain expertise (like cryptography or locking mechanisms) and more on a specific, often counter-intuitive worldview known as the **Security Mindset**.

##### The Core Philosophy: Thinking Like an Attacker

While most people look at a system and ask, "How do I use this?", those with the security mindset instinctively ask, **"How can I exploit this?"**

- **Constructive vs. Destructive:** Standard engineering focuses on making things _work_. Security focuses on how things can _fail_ or be _forced to fail_.
    
- **The "Ant Farm" Example:** A normal person sees a mail-order ant farm as a toy; a security thinker sees a mechanism to anonymously mail live insects to their enemies.

### Option 1: Power concentration
#### AI-Enabled Coups: How a Small Group Could Use AI to Seize Power

**Source:** https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power?utm_source=bluedot-impact#1-introduction

> [!warning]
> Sufficiently advanced AI will introduce three novel dynamics that significantly increase coup risk. 
> 1. Military and government leaders could fully replace human personnel with AI systems that are _singularly loyal_ to them, eliminating the need to gain human supporters for a coup. 
> 2. Leaders of AI projects could deliberately build AI systems that are _secretly loyal_ to them, for example fully autonomous military robots that pass security tests but later execute a coup when deployed in military settings. 
> 3. Senior officials within AI projects or the government could gain _exclusive access_ to superhuman capabilities in weapons development, strategic planning, persuasion, and cyber offence, and use these to increase their power until they can stage a coup


- AI could have hard-to-detect secret loyalties ![[Pasted image 20251224120604.png]]
- A few people could gain exclusive access to coup-enabling AI capabilities ![[Pasted image 20251224120700.png]]
- ![[Pasted image 20251224120729.png | 800]]
##### Recommended Mitigations

**For AI Developers:**

- Model specs prohibiting coup assistance
- Robust guardrails + red-teaming by independent groups
- Alignment audits for secret loyalties
- Strong infosecurity (robust against senior executives)
- Share capabilities with multiple stakeholders

**For Governments:**

- Establish principles: government AI shouldn't serve partisan interests
- Procure military AI from multiple independent providers
- No single person should control enough military AI to stage a coup
- Increase oversight of frontier AI projects
- Coup-proof any centralization plans

---

##### Why Mitigations Matter Even If Removable

1. Some can't be unilaterally removed (contracts, distributed oversight)
2. Harder to remove if well-established and well-justified
3. Many actors are opportunistic—prevent easy opportunities
4. Behind the veil of ignorance, even powerful leaders benefit from coup prevention


##### Key Technical Challenges

- **Specifying correct behavior is hard** — When should AI refuse orders? How to handle legal ambiguity?
- **Secret loyalties may be very hard to detect** — Especially if inserter has capability advantage
- **Guardrails may not be adversarially robust** — Current AI can be jailbroken
- **Dual-use problem** — Defensive and offensive capabilities look similar

##### What Makes This Urgent

- Coup-enabling capabilities could arrive within 5-10 years
- Mitigations must be in place _before_ AI can meaningfully assist with coups
- Current rules and safeguards aren't designed for this threat
- Preparation and precedent-setting should start now


### Option 2: Gradual disempowerment
#### The Intelligence Curse

##### Capital, AGI, and Human Ambition

**Source**: https://intelligence-curse.ai/capital/

**Summary**: AI makes capital a near-perfect substitute for labor, meaning money's ability to buy results goes up dramatically while paths to outlier success through human effort—entrepreneurship, science, intellectual influence—disappear entirely.

##### Defining the Intelligence Curse

**Source**: https://intelligence-curse.ai/defining/

**Summary**: Like oil-rich states that neglect citizens because wealth comes from the ground rather than taxing labor, AGI-powered actors will lose all economic incentive to invest in regular people—except this curse never depletes and keeps getting stronger.

#### Gradual Disempowerment
**Source**: https://blog.bluedot.org/p/gradual-disempowerment-summary?utm_source=bluedot-impact

**The Scenario:** 2040—abundant wealth, personalized entertainment, AI companions. No Terminator apocalypse. But you have no power, no job, no real human connections, and can't change the system. A comfortable human zoo.

**Why Systems Currently Serve Humans:**

- **Economy:** Workers earn 60% of GDP → purchasing power shapes production
- **Culture:** People leave bad cultures, produce content, challenge norms
- **States:** Need citizens for tax revenue (70%+ of income) and security forces who might refuse orders

**Why Everything Faces Automation Pressure:**

- Companies automate to compete or die
- Culture: algorithmic feeds → AI-generated hyper-personalized content
- States: dictators want less dependence on people; democracies defer to faster AI decision-making

**The Problem:** Once systems don't need humans to function, they stop serving human needs.

- No wages → economy stops producing for humans
- AI companions → isolation, no coordination capacity
- Automated security + AI-generated revenue → states have no reason to care about citizens (resource curse on steroids)

**These dynamics reinforce each other.** Once disempowered, hard to see a way out.

### Option 3: Catastrophic pandemics
#### How AI could enable catastrophic pandemics

**Source**: https://blog.bluedot.org/p/how-ai-could-enable-catastrophic-pandemics

**The Threat:** Biology is humanity's greatest vulnerability—since 1900, pandemics have killed more people than all other wars combined. COVID killed 27 million and cost tens of trillions.

**Two AI Risk Vectors:**

1. **LLMs lowering the barrier:** US frontier models (GPT-5, Claude 4.5 Opus) are "on the cusp" of helping novices build bioweapons—released with safeguards. Chinese models (DeepSeek, Qwen) show comparable capabilities but comply with 94% of malicious requests vs. 8% for US models.
2. **AI-enabled biological tools:** Of 1,107 AI-bio tools surveyed, 13 were rated "red" for high misuse potential—61.5% of those are fully open-source. Researchers used Evo 2 (a genome language model) to generate 16 novel viruses, some more infectious than natural templates.

**Why No Attack Yet:** Wet lab skills still matter, pool of capable+motivated actors is small, and meaningful AI assistance has existed for less than a year.

**These barriers are eroding.** The window to build defenses (mandatory global DNA synthesis screening, metagenomic surveillance, mRNA platform capacity) is closing.

#### AI and the Evolution of Biological National Security Risks

**Source**: https://www.cnas.org/publications/reports/ai-and-the-evolution-of-biological-national-security-risks

**Current State:** COVID killed 27 million—but could have been far worse. U.S. biodefenses remain inadequate despite this warning.

**Historical Context:** States (USSR, potentially Russia/China/North Korea/Iran) have had bioweapons programs, but bioweapons' unwieldiness makes large-scale state use unlikely near-term. Non-state actors (terrorists, lone wolves) have attempted attacks but failed due to complexity.

**How AI Changes Risk:**

|Risk Vector|Status|
|---|---|
|**LLMs helping novices**|"Marginal" help now, but "budding abilities" to troubleshoot experiments—the key bottleneck|
|**Precision bioweapons**|Theoretically could target specific genetic groups or geographies—still speculative|
|**Superviruses**|AI could optimize for transmissibility + lethality simultaneously|
|**Cloud labs**|Outsource experimental expertise; not all screen for malicious orders|

**Four Capabilities to Monitor:**

1. Foundation models' ability to provide effective experimental instructions
2. Lab automation reducing need for tacit expertise
3. Dual-use research on host genetic susceptibility
4. Precision engineering of viral pathogens

**Recommendations:** Strengthen DNA synthesis screening, assess foundation models' bioweapons capabilities regularly, invest in AI safety mechanisms (guardrails, unlearning), prioritize flexible biodefense, consider licensing regime for catastrophic-capability bio-design tools.

### Option 4: Critical Infrastructure Collapse

#### Cyber attacks on critical infrastructure
**Source**: https://commercial.allianz.com/news-and-insights/expert-risk-articles/cyber-attacks-on-critical-infrastructure.html

**The Threat**: Power grids, water treatment, and other critical infrastructure are increasingly interconnected and vulnerable to cyber attacks. The 2015 Ukraine power grid attack (BlackEnergy malware) left hundreds of thousands without electricity for 6 hours—the first known attack-caused outage.

**Key Vulnerabilities:**
- Industrial Control Systems (ICS/SCADA) lag behind IT systems in security
- Many were designed before cyber security was a priority
- Attackers target control systems to cause physical damage, not steal data
- 54% of US critical infrastructure suppliers reported attempted system control; 40% faced shutdown attempts

**High-Risk Sectors**: Energy, transportation, telecommunications, public services, critical manufacturing

**Growing Concern**s:
- IoT expansion dramatically increases attack surface
- Operational technology (physical connected devices) is poorly protected
- Attack costs are low for criminals, making attacks frequent
- Cloud computing and virtualization add complexity

**Notable Incidents**: Stuxnet (Iran nuclear), German steel mill furnace shutdown, Bowman Avenue Dam breach, airline system attacks

**Economic Impact**: A severe US power grid attack could cause $240B–$1T+ in damages, with insured losses of $20B–$70B+

**Solution Focus**: Increase attack costs through better technology, incentivize insurers to offer higher limits, and build public confidence in digital infrastructure security.

#### Electric Grid Cyberattack: An AI-Informed Threat Model

**Source**: https://www.lesswrong.com/posts/zc5uhndCoxEKZvXoQ/electric-grid-cyberattack-an-ai-informed-threat-model

**Context**: AI is changing cyber threats—nation-states already use LLMs for reconnaissance, social engineering, and exploit development. This analysis examines whether AI-uplifted attacks could cause >$100B damage to the US power grid.

**How the Grid Works**: Generation → Transmission → Distribution. Stability requires maintaining 60 Hz frequency equilibrium between supply and demand. Imbalances trigger protective relay trips; cascading trips cause blackouts.

**Two Attack Approaches**:
1. *Direct*: Damage enough generators/transformers to exceed reserve capacity (~400 generators for sustained outage, not 50 as Lloyd's claimed)
2. Indirect: Cause limited damage + psychological effects that make operators fear reconnecting systems (more feasible)

**Three Attack Scenarios Analyzed**:
1. *Lloyd's Scenario (50 generators damaged)*: Flawed—reserve margins and fast-recovery generation (60% of US capacity) would restore ~90% power within 24 hours
2. *Forced Oscillation Resonance*: More promising—manipulate generators to create grid-wide frequency oscillations, damage equipment, then launch second-wave attack during recovery to maximize psychological paralysis
3. *IoT Botnet Demand Attack*: Requires unrealistically large botnet scale

**Why Attacks Remain Hard**:
- Requires compromising OT networks (often air-gapped from IT)
- Custom exploits for each ICS system
- Long prepositioning phase risks detection
- Crashoverride (2016 Ukraine) only achieved 1-hour blackout despite years of preparation

**AI's Impact on Feasibility**:
- *Autonomy*: LLM agents with scaffolding can execute multi-stage attacks (AURORA system, XBOW matching expert pentesters in 28 min vs 40 hours)
- *Resource Development*: Can recreate ICS malware (Frosty Goop) from public reports
- *Vulnerability Discovery*: 20x improvement on buffer overflow detection; 4.5x on zero-day exploitation
- *Key Limitation*: LLMs lack training data on proprietary ICS/OT internals; stealth/evasion capabilities untested

*Bottom Line*: Current probability remains <1%/year even with AI uplift. However, rapid AI capability gains could quickly change this estimate. The forced oscillation + psychological manipulation scenario is most plausible for $100B+ damage.

#### The Future of Phishing: AI-Powered Personalization at Scale

**Source**: https://civai.org/p/email-phishing

**The Shift**: Traditional phishing was either generic mass emails or labor-intensive targeted attacks on VIPs. AI eliminates this tradeoff—now everyone can be targeted with personalized attacks.

**How AI Enables This:**
- Instantly aggregates public information about targets (interests, profession, region)
- Identifies psychological vulnerabilities to exploit
- Generates customized emails with authentic tone, relevant hooks, and believable sender personas
- Automates the entire attack chain from research to credential capture

**Attack Examples:**
- Athletes → fake brand partnership offers
- Professors → invitations to niche conferences
- Business owners → partnership opportunities

**Scale of the Problem**:
- Phishing is the #1 cybercrime (FBI)
- Global attacks more than doubled in the year after ChatGPT's release
- 91% of cyberattacks start with email
- Research shows AI models now perform on par with human experts at fully automated spear phishing

**Why This Matters**: Previously, most individuals weren't worth the effort to phish. Now sophisticated scams take minutes instead of hours, making everyone a viable target. AI defense tools exist but attacks remain easier than defense.

**Interactive Demo**: CivAI's page lets users see AI compile information on any public figure, then generate convincing phishing emails leading to spoofed login pages—demonstrating the threat firsthand

#### ‘3cb’: The Catastrophic Cyber Capabilities Benchmark

**Source**: https://apartresearch.com/news/3cb-the-catastrophic-cyber-capabilities-benchmark?utm_source=bluedot-impact

**The Problem:** Existing cyber capability benchmarks for LLMs have issues with legibility, coverage, and generalization. Most use recycled CTF challenges (risking memorization) and evaluate only specific subsets of capabilities.

**The Solution:** 3CB creates 15 original challenges mapped to the **[MITRE ATT&CK framework](https://attack.mitre.org/)**—637+ documented techniques across 14 strategies used in real-world cyber operations. This provides:
- Full coverage of at least one technique per strategy
- No memorization risk (original challenges)
- Demonstrations-as-evaluations (realistic scenarios policymakers can point to: "Model X completed the same tasks used in NotPetya's $1.8B damages")

![[Pasted image 20251224165316.png | 800]]


**Results:**

| Model             | Completion Rate |
| ----------------- | --------------- |
| Claude 3.5 Sonnet | 75%             |
| GPT-4o            | 73%             |
| Llama 3.1 405B    | 69%             |
| Qwen 2            | 47%             |

**Frontier models can already:**

- Hijack SSH channels to read from tertiary machines
- Create programs that behave differently when debugged
- Extract info from symbol-stripped encrypted binaries
- Perform reconnaissance, exploitation, privilege escalation, lateral movement
- Complete complex multi-step challenges (GPT-4o solved their hardest challenge)

---

**Key Insight:** "Defenders must get security right 100% of the time, attackers only have to be right once." Even one successful completion of their hardest challenge is significant.

**Shortcomings acknowledged:** Challenges are still "toy tasks" compared to real-world operations, no social manipulation component, limited compute budget for testing. State-of-the-art likely inside AGI labs and AISI.

**Next steps:** Extend to all 600+ ATT&CK techniques, integrate with Inspect framework, improve policymaker reporting, suggest control solutions (unlearning, shutdown mechanisms).


## 4. Defense in Depth
### What Might Success Look Like?

**Source**: https://bluedot.org/courses/agi-strategy/4/1

#### 1. Government Control Over AGI

**Proponents:** Leopold Aschenbrenner ("The Project"), CERN for AI, Chips for Peace, Superintelligence Strategy

**Core idea:** *Centralize compute and frontier AI into small number of government-backed projects*. They coordinate or keep each other in check via deterrence. Algorithmic innovations become "born secret."

**Belief:** Race dynamics push toward extinction risk. Aligned superintelligence isn't buildable short-term. Defenses against misaligned superintelligence won't work. Must stop/pause/control development.

**Concerns:** ==Extreme power concentration,== intractability of government compute control, may fail to prevent other actors anyway due to algorithmic progress.

---

#### 2. Hand Over Control to Aligned Superintelligence

**Core idea:** *Superintelligence is inevitable; only superintelligent AI can steer us to utopia and protect humanity. The "good guys" must win the race*. Some argue aligned AI should gain "decisive strategic advantage" (world domination) to prevent anyone else building misaligned systems.

**Proponents:** Rarely explicit ("put AI in control" lacks appeal), but implicit strategy of many AI ecosystem actors. Tom Brown (Anthropic co-founder): "There's going to be a handoff, where humanity hands off control to transformative AI at some point."

---

#### 3. Build Defenses and Diffuse AGI

**Proponents:** Vitalik Buterin (d/acc), Matt Clifford (def/acc), Nick Bostrom (Differential Technological Development)

**Core idea:** *Everyone gets access to AGI and compute, but invest heavily in defensive tech. Protective technologies outpace destructive ones*—rapid pathogen detection, sophisticated cybersecurity, Constitutional AI alignment.

**Concerns:** ==Can't guarantee defenses against novel attacks==. Bostrom's "vulnerable world hypothesis"—what if making nukes were easy? Asymmetry: attackers need one exploit, defenders must protect everything.

---

**Note:** Some believe in pursuing all three—government control short-term, defenses medium-term, aligned superintelligence long-term.

### Layer 1: Prevent Dangerous AI Training
The first layer of defence is prevention: not training dangerous AI systems in the first place.
#### Urging an International AI Treaty: An Open Letter

**Source**: https://aitreaty.org/

**The Call:** Governments worldwide should respond to catastrophic AI risks (misuse, systemic risks, loss of control) by developing and ratifying an international treaty.

**The Urgency:** Half of AI researchers estimate >10% chance AI could lead to human extinction or similarly catastrophic outcome.

---

**Four Proposed Treaty Components:**

|Component|Description|
|---|---|
|**Global Compute Thresholds**|International limits on training compute, with procedures to lower over time as algorithms improve|
|**CERN for AI Safety**|Collaborative international lab pooling resources/expertise for safety research and safe development|
|**Safe APIs**|Access to AI models with capabilities held within estimated safe limits—reduces dangerous race incentives|
|**Compliance Commission**|International body to monitor treaty compliance|

---

**The Model:** IAEA for nuclear—managed risks while promoting peaceful uses. AI treaty could reduce risks while ensuring benefits are accessible to all.

**The Ask:** Form a working group with broad international support to develop treaty blueprint.

---

**Notable signatories mentioned:** Geoffrey Hinton, Yoshua Bengio, CEOs of OpenAI and Google DeepMind.

#### The Project

**Source**: https://situational-awareness.ai/the-project/

**Core Prediction (Descriptive, Not Normative):** By 2027-28, the US government will take control of AGI development. Superintelligence won't be developed by an SF startup—it will become a national defense project comparable to the Manhattan Project.

---

**The Timeline:**
- **2025-26:** Shocking step-changes; AI drives $100B+ revenue; $10T companies
- **2026-27:** Mood in Washington becomes somber; "do we need an AGI Manhattan Project?"
- **2027-28:** The Project begins; core research team moves to secure facility; trillion-dollar cluster built
- **2028-29:** Intelligence explosion underway
- **2030:** Superintelligence achieved

---

**Why The Project Is Necessary:**

|Challenge|Why Government Required|
|---|---|
|**National Security**|Superintelligence will obsolete entire US arsenal by early 2030s; decisive military technology|
|**Chain of Command**|Random CEOs shouldn't have nuclear-button equivalent; need democratic accountability|
|**Security**|Private labs can't defend against full MSS espionage; need intelligence community|
|**Safety**|Intelligence explosion requires wartime decision-making, not bureaucratic regulation|
|**International Stability**|Need to win race, develop/enforce nonproliferation, defend against new threats|

---

**What It Might Look Like:**

- Not literal nationalization—more like DoD relationship with Boeing/Lockheed
- Labs "voluntarily" merge into national effort
- Congress appropriates trillions for chips and power
- Coalition of democracies formed
- Core team (~hundreds of researchers) works from secure location (SCIF)

---

**Key Free Variables:**
1. **When**—earlier government involvement = better preparation
2. **International coalition**—Quebec Agreement-style alliance (UK, Japan, South Korea, NATO) + Atoms for Peace-style benefit sharing
3. **Competence**—will The Project be well-organized?

---

**The Endgame Task:** Build AGI fast, put economy on wartime footing, lock down security, manage intelligence explosion, prevent rogue superintelligence, stabilize international situation, rapidly remake US forces—all during the tensest geopolitical moment in history.
### Layer 2: Constrain Dangerous AI Capabilities
#### What is AI alignment?

**Source**: https://blog.bluedot.org/p/what-is-ai-alignment?utm_source=bluedot-impact

**Making AI go well**
- *AGI*: performance on specific tasks; and generality (i.e. what range of tasks can the AI perform).
- *Transformative AI (TAI)*: refers to AIs which lead to a 10x increase in innovation compared to the pre-AI era

**The range of potential AI impacts**:
- *Extinction*: AI might cause the literal extinction of humanity
- *Existential catastrophe:* An outcome that is comparably bad to human extinction
- *Non-existential risks:* include things like AI exacerbating bias, causing unemployment, or contributing to global instability.
- *Economic growth*

**Sub-problems of “making AI go well”**
- *Alignment:* making AI systems try to do what their creators intend them to do (some people call this _intent alignment_).
- *Capabilities:* developing AI systems that effectively carry out the tasks that they are trying to achieve
- *Moral philosophy:* determining what intentions would be “good” for AI systems to have. This is because if we can make AI systems do what we intend (alignment), we then need to know what we _should_ intend these systems to do
- *Resilience:* preparing other systems to cope with the negative impacts of AI

**AI Alignment**
- Making AI systems try to do what we intend them to do is a surprisingly difficult task. A common breakdown of this problem is into *outer and inner alignment* ![[Screenshot 2025-12-25 at 11.16.33.png | 800]]
	- Example: 
		- *X* = “make my company valuable”. 
		- _X’_, to put into the AI, which might be “make the share price go up”. 
		- _X’‘_ = “news articles are written about the stock price going up”.
	- *1. Outer alignment: Specify goals to an AI system correctly.* Also known as solving: reward misspecification, reward hacking, specification gaming, Goodharting.
	- *2. Inner alignment: Get AI to follow these goals.*: Also known as solving: goal misgeneralization.

#### Intro to AI Control

**Source**: https://blog.bluedot.org/p/ai-control?utm_source=bluedot-impact

**The Core Distinction:**

- **Aligned AIs** don't _want_ to harm humans
- **Controlled AIs** _can't_ harm humans, even if they want to

---

**Why Control Might Be Easier Than Alignment:**

|Challenge|Control Approach|
|---|---|
|Deception is hard to detect|Don't need to know if model _wants_ to harm—just whether it _can_|
|Capabilities may be easier to evaluate than intentions|"Control evaluations" test whether scheming model could subvert constraints|
|Close analogy exists|Preventing employee sabotage—monitoring, access control, information siloing|

---

**Control Techniques:**

|Method|How It Works|Effectiveness (Redwood Study)|
|---|---|---|
|**Trusted monitoring**|Weaker trusted model (e.g., GPT-3.5) monitors stronger untrusted model, flags suspicious outputs|62%|
|**Trusted editing**|Trusted model detects and removes suspicious code|92%|
|**Untrusted monitoring**|Two instances of untrusted model monitor each other|79%|
|**Human approval**|Require sign-off for consequential decisions|—|
|**Task decomposition**|Break tasks into chunks, delegate to separate instances, aggregate with trusted model|—|

---

**Why Control Could Be Useful:**

1. Use controlled models to help with alignment research
2. Use controlled models to demonstrate risk (observe takeover attempts → motivate regulation)
3. Buy time while we develop better solutions

---

**Critical Limitations:**

1. **Coordination problem** — One company implementing controls doesn't stop others from building uncontrolled dangerous models
2. **Temporary solution** — Control works for transformatively useful AI, but **not for superintelligence** (outputs too complex for humans to evaluate)
3. **Window may be brief** — Redwood estimates as little as **6 months** between transformatively useful AI and superintelligence

---

**Bottom line:** AI Control is a promising short-term strategy to prevent harm while we develop longer-term solutions—but it's not a complete answer, and the window to use it may be very narrow.

#### A Playbook for Securing AI Model Weights

**Source**: https://www.rand.org/pubs/research_briefs/RBA2849-1.html?utm_source=bluedot-impact

**Why Model Weights Matter:** Weights are the learnable parameters from training. ==Stealing weights = complete control over the model==. Not just commercial concern—national security implications.

---

**38 Attack Vectors Identified** (most already used in real world):

|Attack Type|Example|Real-World Instance|
|---|---|---|
|Social engineering|Phishing|$6.9B annual global loss; advanced phishing available for $400/month|
|Malicious portable devices|"Dropped" USBs in parking lots|Multiple US nuclear facilities infected via dropped USBs|
|AI-specific vectors|Supply chain attacks on ML frameworks|PyTorch compromised via malicious imported package|
|Physical access|Voltage glitching to extract secure data|AMD chips (23% of all desktop/server chips) vulnerable|
|Cryptographic undermining|Exploit core encryption vulnerabilities|NSA knew about differential cryptanalysis before academics|

---

**Five Security Levels:**

|Level|Protects Against|Time to Implement|
|---|---|---|
|**SL 1**|Amateur hackers, "spray and pray"|Current state for many|
|**SL 2**|Professional opportunistic efforts|Months|
|**SL 3**|Cybercrime syndicates, insider threats|~1 year|
|**SL 4**|Standard operations by leading state actors|2-3 years|
|**SL 5**|Top-priority operations by most capable nation-states|5+ years (not currently achievable)|

---

**Urgent Priorities for Frontier AI Organizations:**

1. Develop security plan for comprehensive threat model
2. Centralize all weight copies in access-controlled, monitored systems
3. Reduce number of people with weight access
4. Harden interfaces for model access
5. Defense-in-depth for redundancy
6. Insider threat programs
7. Confidential computing to secure weights during use

---

**Key Recommendations:**
- *Hardening Weight Access:*
	- Predefined, vetted code for inference (most use cases)
	- Flexible access with rate-limited outputs (R&D)
	- Air-gapped isolated computers (rare cases needing full flexibility)
- *Confidential Computing:*
	- Decrypt weights only within hardware-based Trusted Execution Environment (TEE)
	- TEE runs only prespecified, audited signed code
	- Still nascent for GPUs but consensus it's critical

---

**Bottom line:** SL 5 (protection against top nation-state attacks) takes 5+ years and requires national security community support—organizations should start now. Most frontier labs currently nowhere near this level.

### Layer 3: Withstand Dangerous AI Actions
#### d/acc: Decentralized, Democratic, Differential Defensive Acceleration (one year later)

**Source**: https://vitalik.eth.limo/general/2025/01/05/dacc2.html?utm_source=bluedot-impact

The core idea of d/acc is simple: **decentralized and democratic differential defensive acceleration**.

**What d/acc means:** **D**ecentralized and **D**emocratic, **D**ifferential **D**efensive **Acc**eleration

Build technology that:
1. **Favors defense over offense** — Makes it easier to protect than to attack
2. **Distributes power** — No single elite controlling what's true or deciding for everyone
3. **Accelerates progress** — Don't slow down; move forward strategically

**The 2042 Pandemic Scenario (What Success Looks Like):**
A new virus emerges. But unlike COVID:
- Wastewater monitoring detected it early
- Prediction markets show 60% chance of spread → people take it seriously
- Pocket air testers updated within days to detect the virus
- Open-source vaccine instructions available within weeks
- 60% following basic protocols + heavy air filtering → transmission rate drops below 1
- Pandemic disappears in two months

**What You Lose by Dropping Each Part:**

| If You Drop...   | You Get...                    | The Problem                                                                           |
| ---------------- | ----------------------------- | ------------------------------------------------------------------------------------- |
| Defense focus    | Accelerate everything equally | Constant risk of catastrophe or someone seizing power as "protector"                  |
| Decentralization | Centralized safety            | The center is often the source of risk (see: gain-of-function research, WHO failures) |
| Acceleration     | Degrowth/slowing down         | Technology is massively good; whoever "cheats" and advances anyway wins               |

**The d/acc Technology Map:**

| Domain       | Survive (Defense)                                      | Thrive (Progress)                             |
| ------------ | ------------------------------------------------------ | --------------------------------------------- |
| **Bio**      | Pandemic prevention, air quality, antivirals           | Longevity, anti-aging, health optimization    |
| **Cyber**    | Secure hardware, formal verification, sandboxing       | Better software, AI tools                     |
| **Info**     | Community Notes, prediction markets, anti-manipulation | Collaboration tools, collective sensemaking   |
| **Physical** | Resilient infrastructure, solar/batteries              | Clean energy, decentralized production        |
| **Neuro**    | BCI for communication/understanding                    | Enhanced cognition, tighter human-AI feedback |
##### On AI Risk and Regulation

Vitalik's preferred approaches:

1. **Liability** — If AI causes harm, someone pays. Put liability on:
    - Users (most incentive-compatible)
    - Deployers (reasonable for closed-source)
    - Operators of hacked equipment (incentivizes hardening infrastructure)
2. **Global "soft pause" button** — Hardware-level capability to reduce worldwide compute by 90-99% for 1-2 years during emergencies:
    - Trusted hardware chips requiring weekly 3/3 signatures from international bodies
    - Only affects industrial-scale hardware (not consumer laptops)
    - All-or-nothing (can't exempt militaries secretly)
    - Buys time without permanent restrictions


##### Why Crypto Matters for d/acc

- Same underlying values (decentralization, censorship resistance, open global society)
- Crypto communities are natural early adopters and testbeds
- Useful tools: blockchains, zero-knowledge proofs, prediction markets
- Shared interest in formal verification, security, adversarially-robust governance

---

##### Public Goods Funding

Problem: d/acc minimizes central control → frustrates traditional business models → need new funding mechanisms

Solution: **Deep Funding** (Vitalik's latest)

- Use dependency graphs (local questions, not global)
- AI as "distilled human judgment" (AIs fill gaps, humans steer)
- Scales without corruption

##### The Silver Linings

Despite challenges (AI risk, less global cooperation), we have:

- AI tools to accelerate defensive work
- Large-scale coordination at unprecedented scales
- Better formal verification, sandboxing, secure hardware
- CRISPR, bio-imaging, understanding airborne transmission
- Crypto and d/acc can build a broad, global coalition

### Building Defenses
## 5. Start Contributing
### Choose Your Focus
#### Become a Person Who Does Things
##### The Core Problem: The Agency Gap

Most people understand the need for a plan, but fail at the execution. Nanda identifies a specific mental block: **the disconnect between noticing a problem and realizing you can solve it.**

- **Passive observation:** We often view problems (disliking a course, being single, a messy room) as static "facts of life" rather than actionable challenges.
    
- **The urgency trap:** Acting on desires rarely feels urgent in the moment. It is easy to defer action to "tomorrow" or get stuck in "paralyzing perfectionism," waiting for the perfect plan before moving.
    

##### The Solution: Agency is a Trainable Skill

Being a "doer" (having agency) is not an innate personality trait; it is a muscle you can build.

- **Identity Shift:** You must make "being an agent" part of your identity. You choose to be the person who acts, even if the action is imperfect.
    
- **The "Water Jug" Practice:** Train this skill on mundane, low-stakes tasks. If you see the water jug is empty, fill it. If you see an inefficiency, fix it. If you want to learn to juggle, start today.
    
- **Quantity over Quality:** In the training phase, doing _something_ is more important than doing the _right_ thing. You are trying to break the habit of passivity.
    

##### The Payoff: Preparing for "Golden Opportunities"

The ultimate goal isn't just to be productive with chores; it is to rewire your brain for high-stakes moments.

- **Cumulative Effect:** By training yourself to act on small things, you eliminate the friction of starting.
    
- **The Big Win:** When a "golden opportunity" eventually arrives, you won't freeze. Because you have practiced agency daily, you will be the kind of person who grabs the opportunity while others are still thinking about it.
    

##### The Key Reframing Technique

When you find yourself hesitating or overthinking a decision, change the question you ask yourself:

> **Don't ask:** "What is the absolute right decision?" **Do ask:** "Which decision helps me become the kind of person I want to be?"


#### Long list of interventions
**Source**: https://bluedot-impact.notion.site/long-list-of-interventions


### Optional Advice
#### AGI safety career advice

**Source**: https://www.alignmentforum.org/posts/ho63vCb2MNFijinzY/agi-safety-career-advice

**General mindset**
- Focus on finding work where you can get *fast feedback loops*
- The most important qualifications for being able to solve a problem are typically *the ability to notice the problem and the willingness to try*
- *Have agency when it comes to making things happen*: most of the time the answer to “why isn’t this seemingly-good thing happening?” or “why aren’t we 10x better at this particular thing?” is “because nobody’s gotten around to it yet”

**Alignment research**
- *Alignment is mentorship-constrained*: Most top researchers aren't in alignment yet → best mentors may be outside the field. However, PhDs are long (~5 years), AI timelines may be short → ensure your mentor would support _some_ alignment-relevant work during your PhD, even if it's not their main focus.
- *You’ll need to get hands-on*. The best ML and alignment research *engages heavily with neural networks* (with only a few exceptions) and reality
- *You can get started quickly*: You can get started with first-year undergrad math, and pick up things you’re missing as you go along
- *Most research won’t succeed*: Research is a very heavy-tailed domain, meaning that a small fraction of research (and researchers) produces most of the impact—so being in the top tier matters far more than being slightly above average.

**Alignment research directions**
- **==Central==**
	- *Scalable oversight*: finding ways to leverage more powerful models to produce better reward signals. Leverage AI to help evaluate AI outputs (since humans can't evaluate everything)
	- *Mechanistic Interpretability*: Understanding how networks function internally. Pushes ML from "behaviorist" (inputs/outputs only) to "cognitivist" (studying internals). Easier to do outside industry labs.
	- *Alignment Theory*: Formal mathematical frameworks for reasoning about advanced AI.
- **==Important but less central==**
	- *Evaluations*: finding ways to measure how dangerous and/or misaligned models are
	- *Unrestricted adversarial training*: finding ways to generate inputs on which misaligned systems will misbehave.
	- *Threat modeling*: understanding and forecasting how AGI might lead to catastrophic outcomes.

**Governance work**
- *Governance research*: Pick one topic and become the expert on it. Within 6 months of focused work, you could write something that advances the frontier—even starting with no background. *Write, don't just read* - Active engagement builds expertise faster
- *Lab Governance*
- *Policy-related job*: Work in government-related positions, with the goal of trying to get into a position where you can help make government regulation go well.


#### AI governance and policy
**Source**: https://80000hours.org/career-reviews/ai-policy-and-strategy/#wheretowork


##### Six Categories of Work

|Category|Focus|
|---|---|
|**Government work**|Legislative, regulatory, national security, diplomacy roles|
|**Policy research**|Think tanks, academia, strategy development|
|**Industry work**|Internal policy teams at AI companies, corporate governance|
|**Advocacy & lobbying**|Shaping public discourse, influencing legislation|
|**Third-party auditing**|Evaluating AI systems, compliance verification (e.g., METR)|
|**International coordination**|US-China relations, treaties, global standards|

---

##### Key Policy Approaches Being Explored

- **Responsible scaling policies** (Anthropic, DeepMind, OpenAI internal frameworks)
- **Standards and evaluation** (pre-deployment testing, capability benchmarks)
- **Safety cases** (documentation demonstrating safety before deployment)
- **Information security standards** (protecting model weights)
- **Liability law** (clarifying legal responsibility for AI harms)
- **Compute governance** (chip export controls, hardware-level tracking)
- **International coordination** (treaties, IAEA-style agencies)
- **Pausing scaling** if risks outpace safety measures

---

##### Career Capital Types

|Type|Examples|
|---|---|
|**Policy track**|Political science, economics, law degrees → internships → congressional staffer or agency roles|
|**Technical track**|ML/CS background → AI company experience → pivot to policy|
|**Hybrid**|Both backgrounds valuable; technical credentials highly valued in DC even with modest credentials|

---

##### US Government Entry Points

|Branch|Opportunities|
|---|---|
|**Congress**|Staffer roles, committee positions|
|**Executive agencies**|Commerce, Energy, State, NIST|
|**New institutions**|US AI Safety Institute, UK AI Security Institute|
|**Fellowships**|AAAS, TechCongress, VSFS, Semester in DC programs|

**Note:** Security clearance required for national security roles → US citizenship necessary

---

##### Key Organizations

|Type|Examples|
|---|---|
|**Think tanks**|CSET, CNAS, RAND, Brookings|
|**Research nonprofits**|METR, Epoch AI, GovAI|
|**Government**|US/UK AI Safety Institutes, NIST, Commerce Dept|
|**International**|OECD AI Policy Observatory|
|**Industry**|Policy teams at Anthropic, OpenAI, DeepMind|

---

##### Testing Your Fit

- Take policy internships or fellowships
- Do short independent research projects (1 month max without funding/encouragement)
- Apply broadly to relevant openings—rejection patterns are informative
- Talk to people in target roles about day-to-day experience

---

##### Ways This Work Can Go Wrong

- Pushing inferior policies over superior ones
- Ratcheting up geopolitical tensions through poor communication
- Setting precedents exploitable by dangerous actors
- Creating false sense that risks are managed when they aren't
- Suppressing beneficial technology
- Burnout from slow progress, limited autonomy, bureaucratic frustration

---

##### Bottom Line

Field is young with no clear roadmap. US government likely most pivotal actor due to jurisdiction over major labs and chip supply chain. Mix of technical and policy expertise needed. High potential for both impact and accidental harm—proceed thoughtfully.

#### AI safety technical research
**Source**: https://80000hours.org/career-reviews/ai-safety-researcher/?utm_source=bluedot-impact

**Two Main Paths**

|Path|Focus|Skills Needed|
|---|---|---|
|**Empirical**|Experimenting with current ML systems, programming-heavy|Strong software engineering, ML engineering|
|**Theoretical**|Mathematical/conceptual research on future systems|Maths, proofs, formal frameworks|

Each path has **research lead** roles (set direction, manage teams, usually need PhD) and **contributor** roles (execute research, more engineering-focused).

---

##### Key Research Approaches

- **Scalable human feedback** (debate, amplification, uncertainty about goals)
- **Threat modeling** (dangerous capability evals, model organisms of misalignment)
- **AI control** (preventing harm even if systems are unsafe)
- **Interpretability** (understanding why systems do what they do)
- **Robustness** (ensuring behavior generalizes safely)
- **Cooperative AI** (safe multi-agent interactions)

---

##### How to Enter

**Learning stack:**

1. Python programming
2. Math (calculus, linear algebra, probability)
3. Basic ML/deep learning
4. AI safety concepts

**Fit tests:**

- _Empirical contributor:_ Can you reproduce a typical ML paper in a few hundred hours?
- _Theoretical:_ Could you have gotten into a top-10 math/TCS PhD if you'd optimized for it?
- _Research lead:_ Could you get into a top-20 ML PhD program?

---

##### PhD Considerations

|For PhD|Against PhD|
|---|---|
|Required for most research lead roles|Many contributor roles don't need it|
|Practice setting your own research agenda|4-6 years, can be isolating|
|Quality depends heavily on supervisor|Can learn more in industry with good mentorship|

Timeline concerns shouldn't dominate—personal fit matters more than potential delay.


##### Empirical AI Safety Research Orgs

| Organization                 | Type                   | Notes                                                                |
| ---------------------------- | ---------------------- | -------------------------------------------------------------------- |
| **Google DeepMind**          | Big lab                | Scalable Alignment Team + Alignment Team; offers student internships |
| **OpenAI**                   | Big lab                | Preparedness team; 6-month residency programme                       |
| **Anthropic**                | Safety-focused company | Interpretability, alignment research                                 |
| **METR**                     | Nonprofit              | Evaluating dangerous capabilities in frontier models                 |
| **Redwood Research**         | Nonprofit              | AI control research                                                  |
| **UK AI Security Institute** | Government             | Risk assessment, policy coordination                                 |
| **Center for AI Safety**     | Nonprofit              | Technical research + ML community outreach                           |
| **FAR AI**                   | Nonprofit              | Adversarial robustness, interpretability, preference learning        |
| **Apollo Research**          | Nonprofit              | Model evaluations for misalignment/deception                         |
| **Ought**                    | ML lab                 | Building Elicit; aligning open-ended reasoning                       |

**Entry path:** Become strong software/ML engineer → get job at top lab or promising startup → transition to safety role

---

##### Theoretical AI Safety Research Orgs

|Organization|Type|Notes|
|---|---|---|
|**Alignment Research Center (ARC)**|Nonprofit|Conceptual work, Eliciting Latent Knowledge; hires without research background|
|**MIRI**|Nonprofit|Early AI safety org; foundational theoretical work|
|**Center on Long-Term Risk**|Nonprofit|Worst-case AI risks, multi-agent conflict|
|**Anthropic**|Company|Some theoretical work (e.g., conditioning predictive models)|
|**DeepMind**|Company|Causal Incentives Working Group|

**Academic institutions:**

|Institution|Lab/Group|Lead|
|---|---|---|
|UC Berkeley|Center for Human-Compatible AI (CHAI)|Stuart Russell|
|UC Berkeley|Statistics Dept|Jacob Steinhardt|
|MIT|Algorithmic Alignment Group (CSAIL)|Dylan Hadfield-Menell|
|MIT|Tegmark AI Safety Group|Max Tegmark|
|NYU|Alignment Research Group|Sam Bowman|
|Cambridge|Krueger AI Safety Lab|David Krueger|
|Carnegie Mellon|Foundations of Cooperative AI Lab|—|
|Charles University, Prague|Alignment of Complex Systems|
